{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01043f12-0038-4701-9407-3cf21342683d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('../API_KEY', 'r') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7eaa4e7-aa5e-4929-84f3-49d1e2b0ce62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5175\n",
      "page_content='.md\\n.pdf\\ndocument.write(`\\n  <button class=\"theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle\" title=\"light/dark\" aria-label=\"light/dark\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <span class=\"theme-switch\" data-mode=\"light\"><i class=\"fa-solid fa-sun\"></i></span>\\n    <span class=\"theme-switch\" data-mode=\"dark\"><i class=\"fa-solid fa-moon\"></i></span>\\n    <span class=\"theme-switch\" data-mode=\"auto\"><i class=\"fa-solid fa-circle-half-stroke\"></i></span>\\n  </button>\\n`);\\ndocument.write(`\\n  <button class=\"btn btn-sm navbar-btn search-button search-button__button\" title=\"Search\" aria-label=\"Search\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <i class=\"fa-solid fa-magnifying-glass\"></i>\\n  </button>\\n`);\\nDependents\\nDependents#\\nDependents stats for hwchase17/langchain\\n[update: 2023-06-05; only dependent repositories with Stars > 100]\\nRepository\\nStars\\nopenai/openai-cookbook\\n38024\\nLAION-AI/Open-Assistant\\n33609\\nmicrosoft/TaskMatrix\\n33136\\nhpcaitech/ColossalAI\\n30032\\nimartinez/privateGPT\\n28094\\nreworkd/AgentGPT\\n23430\\nopenai/chatgpt-retrieval-plugin\\n17942\\njerryjliu/llama_index\\n16697\\nmindsdb/mindsdb\\n16410\\nmlflow/mlflow\\n14517\\nGaiZhenbiao/ChuanhuChatGPT\\n10793\\ndatabrickslabs/dolly\\n10155\\nopenai/evals' metadata={'id': 'ab037d116dcd-0', 'source': 'https://python.langchain.com/en/latest/dependents.html'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "json_docs = []\n",
    "\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        json_docs.append(json.loads(line))\n",
    "\n",
    "docs = []\n",
    "for doc in json_docs:\n",
    "    text = doc.pop('text')\n",
    "    doc['source'] = doc['source'].replace('rtdocs', 'https:/').replace('\\\\', '/')\n",
    "    docs.append(Document(page_content=text, metadata=doc))\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31fb446-3a9b-47db-b198-6f452933bd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embedder = OpenAIEmbeddings(model='text-embedding-ada-002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "515c115f-f698-49a7-a734-66c480209012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x1b0d25a3340>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raise  # Only re-run this when necessary \n",
    "docstore = FAISS.from_documents(documents=docs, embedding=embedder)\n",
    "# docstore = FAISS.from_documents(documents=docs[0:2], embedding=embedder)\n",
    "# from tqdm.auto import tqdm\n",
    "# for doc in tqdm(docs[2:]):\n",
    "#     docstore.add_documents([doc])\n",
    "docstore.save_local('faiss_dbs', 'langchain_rtd')\n",
    "docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c7b1c4d-e0aa-4a3a-8f1a-d0199e839088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docstore = FAISS.load_local('faiss_dbs', embedder, 'langchain_rtd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9f241c-7e32-4608-bd50-0fdfb961aadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c89b8b4-05cc-4a12-88d6-2d63b489940e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm = OpenAI()\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')  # Stuff all data in at once (Other methods might work better if more context required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ff27330-67d1-4f9f-8186-3ad8df782130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\", template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe56b1a1-414e-4afd-9637-d8615570a3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='chain = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(),\\n    question_generator=question_generator,\\n    combine_docs_chain=doc_chain,\\n)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\nSOURCES: ../../state_of_the_union.txt\"\\nConversationalRetrievalChain with streaming to stdout#\\nOutput from the chain will be streamed to stdout token by token in this example.\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\\nfrom langchain.chains.question_answering import load_qa_chain\\n# Construct a ConversationalRetrievalChain with a streaming llm for combine docs\\n# and a separate, non-streaming llm for question generation\\nllm = OpenAI(temperature=0)\\nstreaming_llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT)\\nqa = ConversationalRetrievalChain(', metadata={'id': '16c17c646fd4-6', 'source': 'https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html'}),\n",
       " Document(page_content='LangChain Workflow for Code Understanding and Generation\\nIndex the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.\\nEmbedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.\\nConstruct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.\\nBuild the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed.\\nAsk questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.\\nThe full tutorial is available below.\\nTwitter the-algorithm codebase analysis with Deep Lake: A notebook walking through how to parse github source code and run queries conversation.\\nLangChain codebase analysis with Deep Lake: A notebook walking through how to analyze and do question answering over THIS code base.\\nprevious\\nQuerying Tabular Data\\nnext\\nInteracting with APIs\\n Contents\\n  \\nConversational Retriever Chain\\nBy Harrison Chase\\n    \\n      Â© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Jun 09, 2023.', metadata={'id': 'ad09bb84952f-1', 'source': 'https://python.langchain.com/en/latest/use_cases/code.html'}),\n",
       " Document(page_content='ConversationalRetrievalChain with map_reduce#\\nWe can also use different types of combine document chains with the ConversationalRetrievalChain chain.\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\\nchain = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(),\\n    question_generator=question_generator,\\n    combine_docs_chain=doc_chain,\\n)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\nConversationalRetrievalChain with Question Answering with sources#\\nYou can also use this chain with the question answering with sources chain.\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")\\nchain = ConversationalRetrievalChain(', metadata={'id': '16c17c646fd4-5', 'source': 'https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html'}),\n",
       " Document(page_content='qa = ConversationalRetrievalChain(\\n    retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\nget_chat_history Function#\\nYou can also specify a get_chat_history function, which can be used to format the chat_history string.\\ndef get_chat_history(inputs) -> str:\\n    res = []\\n    for human, ai in inputs:\\n        res.append(f\"Human:{human}\\\\nAI:{ai}\")\\n    return \"\\\\n\".join(res)\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), get_chat_history=get_chat_history)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\nresult[\\'answer\\']', metadata={'id': '16c17c646fd4-7', 'source': 'https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html'})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the ConversationRetrievalChain do?\"\n",
    "qdocs = docstore.similarity_search(query, k=4)\n",
    "qdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aaf6433-2530-4540-b9dc-3e0ffb76bdf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ConversationalRetrievalChain is a tool that can be used to generate context-aware answers to questions. It searches a VectorStore to identify the most relevant code snippets for a given query and uses an LLM (GPT-4) to generate comprehensive, context-aware answers based on retrieved code snippets and conversation history. It can also be customized with different types of combine document chains and question answering chains.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(input_documents=qdocs, question=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b75ec-ce4b-4164-820c-3f90822335c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
