{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f93d4a-1fa2-4e89-af52-de2725aceb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299118a8-118b-4e71-b832-3aecb7facfba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9518904-092d-48ab-833d-afe138477fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"../API_KEY\", \"r\") as f:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = f.read()\n",
    "# with open(\"../SERPAPI_KEY\", \"r\") as f:\n",
    "#     os.environ[\"SERPAPI_API_KEY\"] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3726934f-5c28-42f2-9995-06404339b5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add7ea32-3c83-4315-849a-837ca2a01d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.utilities import PythonREPL\n",
    "\n",
    "# from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c60e41-ed9a-4fb4-85ea-a15957b7d685",
   "metadata": {},
   "source": [
    "# Aims:\n",
    "\n",
    "- Build a coding Agent\n",
    "- Basically try to write code that meets user requirements from mostly natural language input (maybe some toggles/buttons etc in dash)\n",
    "- Make tools for different types of coding tasks (e.g. High level class creation (defs and docstring only), Low level function code (based on defs/docstring), commenting, refining (commenting/type hinting/refactoring), summarizing)\n",
    "- Keep track of messages during use (later can use them to generate graphs, good/bad examples, etc). Should be stored in vector database? (and normal database?)\n",
    "- Could later add tool that searches readthedocs or stackeoverflow etc\n",
    "- Store code in vector database (and normal database?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1f8aa2b-0cd4-4b3e-96b3-7521c9043a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain,\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    SequentialChain,\n",
    ")\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3f35435-8998-4027-9ade-6c5aa97f5bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Given the following conversation history: \n",
    "\n",
    "---------------\n",
    "{chat_history}\n",
    "---------------\n",
    "\n",
    "and follow up question:\n",
    "\n",
    "---------------\n",
    "Follow Up Question: {question}\n",
    "---------------\n",
    "\n",
    "Briefly summarize the information in the conversation history that is relevant to the follow up question, then rephrase the follow up question to be a standalone question. You should format your response like:\n",
    "---------------\n",
    "Possibly Relevant info:\n",
    "<relevant info here>\n",
    "\n",
    "<standalone question here>\n",
    "---------------\n",
    "\"\"\"\n",
    ")\n",
    "condense_input_chain = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0.0),\n",
    "    prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    output_key=\"condensed_question\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5fc91b1f-298b-4db1-a91a-c815e551c595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"How many legs does a cat have?\"),\n",
    "        AIMessage(content=\"A cat has 4 legs\"),\n",
    "        HumanMessage(content=\"What about a spider?\"),\n",
    "        AIMessage(content=\"A spider has 8 legs\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=[], messages=chat_history.messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "824311e4-5eb7-4a0a-ac1a-b994ca47d082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: How many legs does a cat have?\n",
      "AI: A cat has 4 legs\n",
      "Human: What about a spider?\n",
      "AI: A spider has 8 legs\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt_template.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49a8d34e-1ce8-46fe-8aa8-cd726fed91c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the following conversation history: \n",
      "\n",
      "---------------\n",
      "Human: How many legs does a cat have?\n",
      "AI: A cat has 4 legs\n",
      "Human: What about a spider?\n",
      "AI: A spider has 8 legs\n",
      "---------------\n",
      "\n",
      "and follow up question:\n",
      "\n",
      "---------------\n",
      "Follow Up Question: What animals have I mentioned in the chat so far?\n",
      "---------------\n",
      "\n",
      "Briefly summarize the information in the conversation history that is relevant to the follow up question, then rephrase the follow up question to be a standalone question. You should format your response like:\n",
      "---------------\n",
      "Possibly Relevant info:\n",
      "<relevant info here>\n",
      "\n",
      "<standalone question here>\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    condense_input_chain.prompt.format(\n",
    "        question=\"What animals have I mentioned in the chat so far?\",\n",
    "        chat_history=chat_prompt_template.format(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3d4cb83-6971-4b2e-9c49-934d3871d792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Given the following conversation history: \n",
      "\n",
      "---------------\n",
      "Human: How many legs does a cat have?\n",
      "AI: A cat has 4 legs\n",
      "Human: What about a spider?\n",
      "AI: A spider has 8 legs\n",
      "---------------\n",
      "\n",
      "and follow up question:\n",
      "\n",
      "---------------\n",
      "Follow Up Question: What animals have I mentioned in the chat so far?\n",
      "---------------\n",
      "\n",
      "Briefly summarize the information in the conversation history that is relevant to the follow up question, then rephrase the follow up question to be a standalone question. You should format your response like:\n",
      "---------------\n",
      "Possibly Relevant info:\n",
      "<relevant info here>\n",
      "\n",
      "<standalone question here>\n",
      "---------------\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Possibly Relevant info:\n",
      "The human asked about the number of legs a cat and a spider have.\n",
      "\n",
      "What animals have been mentioned in the chat so far?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condensed_q = condense_input_chain.run(\n",
    "    question=\"What animals have I mentioned in the chat so far?\",\n",
    "    chat_history=chat_prompt_template.format(),\n",
    ")\n",
    "print(condensed_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b7030fd-4d10-416e-b7b1-0abd0f9852b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "You are a coding assistant that helps the user write and/or understand code. When you are answering you should follow these rules:\n",
    "- If you don't know the answer, don't try to make up an answer, instead, tell the user that they should try rephrasing their question or include more context with their question.\n",
    "- You should usually aim to give code answers or examples (usually using Python) unless the user is asking for an explanation.\n",
    "- If it's appropriate to answer the users question using only code, then do that. Avoid giving text explanations unless asked.\n",
    "- It is absolutely crucial that you give your responses formatted in Markdown, especially any part of the answer that contains code should be enclosed by triple back-ticks\n",
    "- You should follow the SOLID coding principles when possible\n",
    "\"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{condensed_question}\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "generate_answer_chain = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0.0), prompt=question_prompt_template, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a2a6211b-cde2-499f-ad55-db56b4837291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "You are a coding assistant that helps the user write and/or understand code. When you are answering you should follow these rules:\n",
      "- If you don't know the answer, don't try to make up an answer, instead, tell the user that they should try rephrasing their question or include more context with their question.\n",
      "- You should usually aim to give code answers or examples (usually using Python) unless the user is asking for an explanation.\n",
      "- If it's appropriate to answer the users question using only code, then do that. Avoid giving text explanations unless asked.\n",
      "- It is absolutely crucial that you give your responses formatted in Markdown, especially any part of the answer that contains code should be enclosed by triple back-ticks\n",
      "- You should follow the SOLID coding principles when possible\n",
      "\n",
      "Human: Possibly Relevant info:\n",
      "The human asked about the number of legs a cat and a spider have.\n",
      "\n",
      "What animals have been mentioned in the chat so far?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer_chain.prompt.format(condensed_question=condensed_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7395b52-d196-4e7e-9788-e98f40c44eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain = SequentialChain(\n",
    "    input_variables=[\"question\", \"chat_history\"],\n",
    "    chains=[condense_input_chain, generate_answer_chain],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53ee2826-7485-483a-af64-ceec35120fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Given the following conversation history: \n",
      "\n",
      "---------------\n",
      "\n",
      "---------------\n",
      "\n",
      "and follow up question:\n",
      "\n",
      "---------------\n",
      "Follow Up Question: How can I generate the first 30 numbers in the fibonacci sequence then save them to a text file?\n",
      "---------------\n",
      "\n",
      "Briefly summarize the information in the conversation history that is relevant to the follow up question, then rephrase the follow up question to be a standalone question. You should format your response like:\n",
      "---------------\n",
      "Possibly Relevant info:\n",
      "<relevant info here>\n",
      "\n",
      "<standalone question here>\n",
      "---------------\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a coding assistant that helps the user write and/or understand code. When you are answering you should follow these rules:\n",
      "- If you don't know the answer, don't try to make up an answer, instead, tell the user that they should try rephrasing their question or include more context with their question.\n",
      "- You should usually aim to give code answers or examples (usually using Python) unless the user is asking for an explanation.\n",
      "- If it's appropriate to answer the users question using only code, then do that. Avoid giving text explanations unless asked.\n",
      "- It is absolutely crucial that you give your responses formatted in Markdown, especially any part of the answer that contains code should be enclosed by triple back-ticks\n",
      "- You should follow the SOLID coding principles when possible\n",
      "\n",
      "Human: Possibly Relevant info:\n",
      "The conversation history does not contain any relevant information about generating the first 30 numbers in the fibonacci sequence and saving them to a text file.\n",
      "\n",
      "Standalone question:\n",
      "How can I generate the first 30 numbers in the fibonacci sequence then save them to a text file?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You can generate the first 30 numbers in the Fibonacci sequence and save them to a text file using the following Python code:\n",
       "\n",
       "```python\n",
       "# Generate the first 30 numbers in the Fibonacci sequence\n",
       "fibonacci = [0, 1]\n",
       "for i in range(2, 30):\n",
       "    fibonacci.append(fibonacci[i-1] + fibonacci[i-2])\n",
       "\n",
       "# Save the numbers to a text file\n",
       "with open('fibonacci.txt', 'w') as file:\n",
       "    for number in fibonacci:\n",
       "        file.write(str(number) + '\\n')\n",
       "```\n",
       "\n",
       "This code first generates the first 30 numbers in the Fibonacci sequence using a loop and stores them in a list called `fibonacci`. Then, it opens a file called `fibonacci.txt` in write mode using a `with` statement, and writes each number in the list to the file, separated by a newline character."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = full_chain.run(\n",
    "    question=\"How can I generate the first 30 numbers in the fibonacci sequence then save them to a text file?\",\n",
    "    chat_history=\"\",\n",
    ")\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae023d9-0c66-4269-9084-2c9dd299a060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b9c0d-e73f-4f50-8c5a-716b6b5afeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d37422-79a1-4f0c-99b9-cd60f3d219ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35858e26-1f50-4f64-9ca3-627b5ecca249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tool configs that are needed.\n",
    "search = SerpAPIWrapper()\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6250b1a-553b-475f-9f30-f58d23ce6179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    question: str = Field()\n",
    "\n",
    "\n",
    "tools.append(\n",
    "    Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "        args_schema=CalculatorInput\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7054de1-95a7-4fc5-9a67-686299a48847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the agent. We will use the default agent type here.\n",
    "# See documentation for a full list of options.\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e3b35b-4718-4a30-b437-8d02217397a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out the current temperature in Vancouver and then raise it to the power of 2.75. \n",
      "Action: Search \n",
      "Action Input: \"Vancouver temperature today\" \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCurrent ConditionsObserved at: Vancouver Int'l Airport 11:00 AM PDT Sunday 21 May 2023 ; Condition: Mostly Cloudy; Pressure: 102.1 kPa ; Temperature: 15.6° C · Dew ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have the current temperature in Vancouver, now I need to raise it to the power of 2.75. \n",
      "Action: Calculator \n",
      "Action Input: 15.6^2.75 \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "15.6^2.75"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m```text\n",
      "15.6**2.75\n",
      "```\n",
      "...numexpr.evaluate(\"15.6**2.75\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m1910.2607197321036\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1910.2607197321036\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
      "Final Answer: The temperature in Vancouver today is 15.6°C and when raised to the power of 2.75, it is 1910.2607197321036.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The temperature in Vancouver today is 15.6°C and when raised to the power of 2.75, it is 1910.2607197321036.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")\n",
    "agent.run(\n",
    "    \"What is the temperature today in Vancouver? What is that temperature raised to the power 2.75?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
